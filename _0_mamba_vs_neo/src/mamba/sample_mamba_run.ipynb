{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:49.731048Z",
     "start_time": "2024-05-25T21:37:47.750852Z"
    }
   },
   "source": [
    "from _0_mamba_vs_neo.models.MambaForSequenceClassification import MambaForSequenceClassification\n",
    "import _0_mamba_vs_neo.datasets.ecthr.utils_ecthr as utils_ecthr"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:50.220671Z",
     "start_time": "2024-05-25T21:37:49.732104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, hamming_loss\n",
    "import os"
   ],
   "id": "9fb2a38aeb33a5e2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:50.223631Z",
     "start_time": "2024-05-25T21:37:50.221526Z"
    }
   },
   "cell_type": "code",
   "source": "os.environ[\"WANDB_PROJECT\"] = \"mamba_vs_neo\"",
   "id": "e432f5a75cfa04ed",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:50.234227Z",
     "start_time": "2024-05-25T21:37:50.224338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "CONFIGS:\n",
    "\"\"\""
   ],
   "id": "7302d5b13c10aea0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCONFIGS:\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:50.241490Z",
     "start_time": "2024-05-25T21:37:50.235482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    general:\n",
    "        - RUN_NAME: str\n",
    "            name of the run\n",
    "        - OUTPUT_DIR: str\n",
    "            directory to save the model and logs\n",
    "        - SEED: int\n",
    "            random seed to use\n",
    "        - REPORT_TO: str\n",
    "\"\"\"\n",
    "RUN_NAME = \"sample_mamba_run\"\n",
    "OUTPUT_DIR = f\"_0_mamba_vs_neo/models/mamba/{RUN_NAME}\"\n",
    "SEED = 42\n",
    "REPORT_TO = \"wandb\""
   ],
   "id": "89a6f0a4fc3c0f0d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:57:57.800542Z",
     "start_time": "2024-05-26T09:57:57.796943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    dataset:\n",
    "        - ALLEGATIONS: bool\n",
    "            True: use allegation data for the cases, so what laws did the cases allegedly violate\n",
    "            False: use court decisions, so what laws did the court decide the cases violated\n",
    "        - SILVER: bool\n",
    "            True: only use facts which were deemed relevant by the court\n",
    "            False: use all facts\n",
    "        - MULTI_LABEL: bool\n",
    "            True: use multi-label classification (which law was (allegedly) violated)\n",
    "            False: use binary classification (was there a law (allegedly) violated)\n",
    "        - FREQUENCY_THRESHOLD: int\n",
    "            minimum number of cases a law must be (allegedly) violated in to be considered\n",
    "        - NUM_LABELS: int\n",
    "            number of labels in the dataset (ecthr: 41)\n",
    "        - MAX_LENGTH: int\n",
    "            maximum number of tokens in a sequence     \n",
    "\"\"\"\n",
    "ALLEGATIONS = True\n",
    "SILVER = True\n",
    "MULTI_LABEL = True\n",
    "FREQUENCY_THRESHOLD = 0\n",
    "NUM_LABELS = 41\n",
    "\n",
    "MAX_LENGTH = 512"
   ],
   "id": "6befe586279fd39c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:50.258099Z",
     "start_time": "2024-05-25T21:37:50.249837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    training:\n",
    "        - EPOCHS: int\n",
    "            number of times to iterate over the dataset\n",
    "        - LEARNING_RATE: float\n",
    "            rate at which the model learns\n",
    "        - BATCH_SIZE: int\n",
    "            number of sequences in a batch\n",
    "        - GRADIENT_ACCUMULATION_STEPS: int\n",
    "            number of batches to accumulate gradients over\n",
    "        - USE_LENGTH_GROUPING: bool\n",
    "            True: group sequences of similar length together to minimize padding\n",
    "            False: do not group sequences by length\n",
    "        - WARMUP_RATIO: float\n",
    "            ratio of training steps to warmup steps\n",
    "        - MAX_GRAD_NORM: float\n",
    "            maximum gradient norm to clip to\n",
    "        - WEIGHT_DECAY: float\n",
    "            weight decay to apply to the model\n",
    "\"\"\"\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "print(\"true batch size:\", BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)\n",
    "\n",
    "WARMUP_RATIO = 0.1\n",
    "MAX_GRAD_NORM = 0.3\n",
    "WEIGHT_DECAY = 0.001\n",
    "\n",
    "USE_LENGTH_GROUPING = True"
   ],
   "id": "893de6da13f235d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true batch size: 16\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:50.266407Z",
     "start_time": "2024-05-25T21:37:50.258847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    evaluation:\n",
    "        - EVAL_STEPS: int\n",
    "            number of steps between evaluations\n",
    "        - BATCH_SIZE_EVAL: int\n",
    "            number of sequences in a batch for evaluation\n",
    "        - LOGGING_STEPS: int\n",
    "            number of steps between logging\n",
    "        - EVAL_ACCUMULATION_STEPS: int\n",
    "            number eval batches to calculate before copying to the cpu, if the eval requires a lot of memory this is helpful\n",
    "\"\"\"\n",
    "EVAL_STEPS = 200\n",
    "BATCH_SIZE_EVAL = BATCH_SIZE\n",
    "LOGGING_STEPS = 100\n",
    "EVAL_ACCUMULATION_STEPS = 20"
   ],
   "id": "baba926e7f85c13b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:50.274537Z",
     "start_time": "2024-05-25T21:37:50.267159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    model:\n",
    "        - MODEL_NAME: str\n",
    "            name of the model to use\n",
    "        - LORA_TASK_TYPE:\n",
    "        - LORA_R: int\n",
    "           r is the rank of the approximation\n",
    "        - LORA_TARGET_MODULES: list\n",
    "            list of modules to target with LoRA\n",
    "\"\"\"\n",
    "MODEL_NAME = \"state-spaces/mamba-1.4b-hf\"\n",
    "LORA_TASK_TYPE = TaskType.SEQ_CLS\n",
    "LORA_R = 8\n",
    "LORA_TARGET_MODULES = [\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"]"
   ],
   "id": "a1937d47960adb9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:50.282349Z",
     "start_time": "2024-05-25T21:37:50.275287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # TODO: understand better, why logits is not a tuple (predictions_i_think, logits) ? why did this change\n",
    "    print(logits)\n",
    "    print(logits.shape)\n",
    "    \n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    predictions = (probs > 0.5).astype(int)\n",
    "    \n",
    "    precision_macro, recall_macto, f1_macro, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(labels, predictions, average='micro')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        'strict_accuracy': accuracy,\n",
    "        'hamming_accuracy': 1 - hamming_loss(labels, predictions),\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'precision_micro': precision_micro,\n",
    "        'recall_macro': recall_macto,\n",
    "        'recall_micro': recall_micro\n",
    "    }"
   ],
   "id": "ca5335c6d3836d38",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:50.290261Z",
     "start_time": "2024-05-25T21:37:50.283090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleBCELossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits, labels.float())\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "id": "a5429b5b3fa7540c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:51.547873Z",
     "start_time": "2024-05-25T21:37:50.290980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = MambaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-1.4b-hf\")"
   ],
   "id": "6267f525c2c7421f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "564b08a504d54d578d0853729507ffef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MambaForSequenceClassification were not initialized from the model checkpoint at state-spaces/mamba-1.4b-hf and are newly initialized: ['backbone.classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:51.550759Z",
     "start_time": "2024-05-25T21:37:51.548625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.pad_token_id = tokenizer.eos_token_id"
   ],
   "id": "58d4063fa0ee2176",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:51.559926Z",
     "start_time": "2024-05-25T21:37:51.552622Z"
    }
   },
   "cell_type": "code",
   "source": "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding = True)",
   "id": "8b1592b4547f7fc9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:58:17.732050Z",
     "start_time": "2024-05-26T09:58:07.739295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ecthr_dataset = utils_ecthr.load_ecthr_dataset(allegations=ALLEGATIONS, silver=SILVER, is_multi_label=MULTI_LABEL, frequency_threshold=FREQUENCY_THRESHOLD)\n",
    "ecthr_dataset = utils_ecthr.tokenize_dataset(ecthr_dataset, tokenizer, max_length=MAX_LENGTH)\n",
    "ecthr_dataset = ecthr_dataset.remove_columns(\"facts\")"
   ],
   "id": "cc2df7d0da5e4328",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea50482eab284e3581a0f8ff91814237"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "897590b00d9b47c3a3f8c0e96cec8d54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e79365194b1149f9907f4fe49bec5638"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:58:20.043889Z",
     "start_time": "2024-05-26T09:58:20.040850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = ecthr_dataset[\"train\"]\n",
    "val = ecthr_dataset[\"validation\"]\n",
    "test = ecthr_dataset[\"test\"]"
   ],
   "id": "43a3d065e7e80583",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:54.717371Z",
     "start_time": "2024-05-25T21:37:54.710486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lora_config =  LoraConfig(\n",
    "        r=LORA_R,\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        task_type=LORA_TASK_TYPE,\n",
    "        bias=\"none\"\n",
    ")"
   ],
   "id": "79702f6a279249d1",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:54.845469Z",
     "start_time": "2024-05-25T21:37:54.718291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ],
   "id": "56fe4e927b2759d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,428,352 || all params: 1,380,690,752 || trainable%: 0.6104\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:55.618204Z",
     "start_time": "2024-05-25T21:37:54.846322Z"
    }
   },
   "cell_type": "code",
   "source": "model.to(\"cuda\")",
   "id": "bccf92a23ac3a46",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MambaForSequenceClassification(\n",
       "      (embeddings): lora.Embedding(\n",
       "        (base_layer): Embedding(50280, 2048)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict()\n",
       "        (lora_B): ModuleDict()\n",
       "        (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 8x50280 (cuda:0)])\n",
       "        (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 2048x8 (cuda:0)])\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-47): 48 x MambaBlock(\n",
       "          (norm): MambaRMSNorm()\n",
       "          (mixer): MambaMixer(\n",
       "            (conv1d): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(3,), groups=4096)\n",
       "            (act): SiLU()\n",
       "            (in_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (x_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=4096, out_features=160, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=160, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (dt_proj): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (out_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_f): MambaRMSNorm()\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=2048, out_features=41, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=2048, out_features=41, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T21:37:55.625525Z",
     "start_time": "2024-05-25T21:37:55.619024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ],
   "id": "cbc50cb119f59b2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.embeddings.lora_embedding_A.default\n",
      "base_model.model.embeddings.lora_embedding_B.default\n",
      "base_model.model.layers.0.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.0.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.0.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.0.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.0.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.0.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.1.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.1.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.1.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.1.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.1.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.1.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.2.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.2.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.2.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.2.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.2.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.2.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.3.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.3.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.3.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.3.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.3.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.3.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.4.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.4.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.4.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.4.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.4.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.4.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.5.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.5.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.5.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.5.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.5.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.5.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.6.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.6.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.6.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.6.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.6.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.6.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.7.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.7.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.7.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.7.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.7.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.7.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.8.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.8.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.8.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.8.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.8.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.8.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.9.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.9.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.9.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.9.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.9.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.9.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.10.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.10.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.10.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.10.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.10.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.10.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.11.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.11.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.11.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.11.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.11.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.11.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.12.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.12.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.12.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.12.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.12.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.12.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.13.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.13.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.13.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.13.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.13.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.13.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.14.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.14.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.14.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.14.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.14.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.14.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.15.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.15.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.15.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.15.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.15.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.15.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.16.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.16.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.16.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.16.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.16.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.16.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.17.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.17.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.17.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.17.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.17.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.17.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.18.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.18.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.18.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.18.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.18.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.18.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.19.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.19.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.19.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.19.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.19.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.19.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.20.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.20.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.20.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.20.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.20.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.20.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.21.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.21.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.21.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.21.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.21.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.21.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.22.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.22.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.22.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.22.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.22.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.22.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.23.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.23.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.23.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.23.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.23.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.23.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.24.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.24.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.24.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.24.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.24.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.24.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.25.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.25.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.25.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.25.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.25.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.25.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.26.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.26.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.26.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.26.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.26.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.26.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.27.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.27.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.27.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.27.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.27.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.27.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.28.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.28.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.28.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.28.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.28.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.28.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.29.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.29.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.29.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.29.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.29.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.29.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.30.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.30.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.30.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.30.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.30.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.30.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.31.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.31.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.31.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.31.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.31.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.31.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.32.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.32.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.32.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.32.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.32.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.32.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.33.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.33.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.33.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.33.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.33.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.33.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.34.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.34.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.34.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.34.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.34.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.34.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.35.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.35.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.35.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.35.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.35.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.35.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.36.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.36.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.36.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.36.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.36.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.36.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.37.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.37.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.37.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.37.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.37.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.37.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.38.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.38.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.38.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.38.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.38.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.38.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.39.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.39.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.39.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.39.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.39.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.39.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.40.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.40.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.40.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.40.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.40.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.40.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.41.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.41.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.41.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.41.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.41.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.41.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.42.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.42.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.42.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.42.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.42.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.42.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.43.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.43.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.43.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.43.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.43.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.43.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.44.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.44.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.44.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.44.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.44.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.44.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.45.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.45.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.45.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.45.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.45.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.45.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.46.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.46.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.46.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.46.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.46.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.46.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.layers.47.mixer.in_proj.lora_A.default.weight\n",
      "base_model.model.layers.47.mixer.in_proj.lora_B.default.weight\n",
      "base_model.model.layers.47.mixer.x_proj.lora_A.default.weight\n",
      "base_model.model.layers.47.mixer.x_proj.lora_B.default.weight\n",
      "base_model.model.layers.47.mixer.out_proj.lora_A.default.weight\n",
      "base_model.model.layers.47.mixer.out_proj.lora_B.default.weight\n",
      "base_model.model.classifier.modules_to_save.default.weight\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:58:41.860462Z",
     "start_time": "2024-05-26T09:58:41.826581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= OUTPUT_DIR,\n",
    "    run_name= RUN_NAME,\n",
    "    learning_rate= LEARNING_RATE,\n",
    "    lr_scheduler_type= \"constant\",\n",
    "    warmup_ratio= WARMUP_RATIO,\n",
    "    max_grad_norm= MAX_GRAD_NORM,\n",
    "    per_device_train_batch_size= BATCH_SIZE,\n",
    "    per_device_eval_batch_size= BATCH_SIZE_EVAL,\n",
    "    gradient_accumulation_steps= GRADIENT_ACCUMULATION_STEPS,#\n",
    "    group_by_length= USE_LENGTH_GROUPING,\n",
    "    num_train_epochs= EPOCHS,\n",
    "    weight_decay= WEIGHT_DECAY,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps= EVAL_STEPS,\n",
    "    eval_accumulation_steps = EVAL_ACCUMULATION_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps= EVAL_STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to= REPORT_TO,\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps= LOGGING_STEPS,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ],
   "id": "4aea53a3acaa584f",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:58:42.968720Z",
     "start_time": "2024-05-26T09:58:42.965958Z"
    }
   },
   "cell_type": "code",
   "source": "#small_val = val.shuffle(seed=SEED).select(range(100))",
   "id": "ba6b747ad55b5a18",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:58:44.448977Z",
     "start_time": "2024-05-26T09:58:44.436965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = SimpleBCELossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "33280cfe33e287e1",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T10:00:12.580157Z",
     "start_time": "2024-05-26T09:58:45.315074Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "db97522ffc2fbbd2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/efi/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/efi/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1625' max='1686' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1625/1686 01:13 < 03:16, 0.31 it/s, Epoch 2.89/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacity of 9.76 GiB of which 186.50 MiB is free. Process 14850 has 16.10 MiB memory in use. Including non-PyTorch memory, this process has 9.15 GiB memory in use. Of the allocated memory 8.51 GiB is allocated by PyTorch, and 384.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/transformers/trainer.py:1885\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1883\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   1884\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1885\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1886\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1887\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1888\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1889\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1890\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/transformers/trainer.py:2216\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2213\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   2215\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 2216\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2219\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2220\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2221\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2222\u001B[0m ):\n\u001B[1;32m   2223\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2224\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/transformers/trainer.py:3238\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   3235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   3237\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 3238\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3240\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m   3241\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "Cell \u001B[0;32mIn[11], line 4\u001B[0m, in \u001B[0;36mSimpleBCELossTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_loss\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, inputs, return_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m      3\u001B[0m     labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     logits \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m     loss_fct \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mBCEWithLogitsLoss()\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/peft/peft_model.py:1238\u001B[0m, in \u001B[0;36mPeftModelForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[0m\n\u001B[1;32m   1236\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m peft_config\u001B[38;5;241m.\u001B[39mpeft_type \u001B[38;5;241m==\u001B[39m PeftType\u001B[38;5;241m.\u001B[39mPOLY:\n\u001B[1;32m   1237\u001B[0m             kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtask_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m task_ids\n\u001B[0;32m-> 1238\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1239\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1240\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1241\u001B[0m \u001B[43m            \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1242\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1243\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1244\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1245\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1246\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1247\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1249\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m _get_batch_size(input_ids, inputs_embeds)\n\u001B[1;32m   1250\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1251\u001B[0m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:179\u001B[0m, in \u001B[0;36mBaseTuner.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any):\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/_0_mamba_vs_neo/models/MambaForSequenceClassification.py:41\u001B[0m, in \u001B[0;36mMambaForSequenceClassification.forward\u001B[0;34m(self, input_ids, inputs_embeds, cache_params, use_cache, output_hidden_states, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     31\u001B[0m         input_ids: Optional[torch\u001B[38;5;241m.\u001B[39mLongTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m     38\u001B[0m ):\n\u001B[1;32m     39\u001B[0m     return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m---> 41\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state\n\u001B[1;32m     50\u001B[0m     last_non_pad_positions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_find_last_non_pad_position(input_ids)\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:574\u001B[0m, in \u001B[0;36mMambaModel.forward\u001B[0;34m(self, input_ids, inputs_embeds, cache_params, use_cache, output_hidden_states, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m    572\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mixer_block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m    573\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_checkpointing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m--> 574\u001B[0m         hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gradient_checkpointing_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmixer_block\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    576\u001B[0m         hidden_states \u001B[38;5;241m=\u001B[39m mixer_block(hidden_states, cache_params\u001B[38;5;241m=\u001B[39mcache_params)\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/_compile.py:24\u001B[0m, in \u001B[0;36m_disable_dynamo.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dynamo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecursive\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:489\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    487\u001B[0m     dynamo_config_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 489\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    491\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001B[0m, in \u001B[0;36mwrap_inline.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:482\u001B[0m, in \u001B[0;36mcheckpoint\u001B[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001B[0m\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m context_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m noop_context_fn \u001B[38;5;129;01mor\u001B[39;00m debug \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    478\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    479\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing `context_fn` or `debug` is only supported when \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    480\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_reentrant=False.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    481\u001B[0m         )\n\u001B[0;32m--> 482\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mCheckpointFunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreserve\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    484\u001B[0m     gen \u001B[38;5;241m=\u001B[39m _checkpoint_without_reentrant_generator(\n\u001B[1;32m    485\u001B[0m         function, preserve, context_fn, determinism_check, debug, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    486\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/autograd/function.py:553\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    552\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[1;32m    556\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    557\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    558\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    559\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    560\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    561\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:261\u001B[0m, in \u001B[0;36mCheckpointFunction.forward\u001B[0;34m(ctx, run_function, preserve_rng_state, *args)\u001B[0m\n\u001B[1;32m    258\u001B[0m ctx\u001B[38;5;241m.\u001B[39msave_for_backward(\u001B[38;5;241m*\u001B[39mtensor_inputs)\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 261\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mrun_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:341\u001B[0m, in \u001B[0;36mMambaBlock.forward\u001B[0;34m(self, hidden_states, cache_params)\u001B[0m\n\u001B[1;32m    338\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresidual_in_fp32:\n\u001B[1;32m    339\u001B[0m     residual \u001B[38;5;241m=\u001B[39m residual\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m--> 341\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmixer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    342\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:305\u001B[0m, in \u001B[0;36mMambaMixer.forward\u001B[0;34m(self, hidden_states, cache_params)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states, cache_params: Optional[MambaCache] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_fast_path_available \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_proj\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype:\n\u001B[0;32m--> 305\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda_kernels_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslow_forward(hidden_states, cache_params)\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:152\u001B[0m, in \u001B[0;36mMambaMixer.cuda_kernels_forward\u001B[0;34m(self, hidden_states, cache_params)\u001B[0m\n\u001B[1;32m    149\u001B[0m projected_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj(hidden_states)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;129;01mand\u001B[39;00m cache_params \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# Doesn't support outputting the states -> used for training\u001B[39;00m\n\u001B[0;32m--> 152\u001B[0m     contextualized_states \u001B[38;5;241m=\u001B[39m \u001B[43mmamba_inner_fn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprojected_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1d\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1d\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_conv_bias\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdt_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_bias\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mA_log\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# input-dependent B\u001B[39;49;00m\n\u001B[1;32m    162\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# input-dependent C\u001B[39;49;00m\n\u001B[1;32m    163\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdelta_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdt_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdelta_softplus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     hidden_states, gate \u001B[38;5;241m=\u001B[39m projected_states\u001B[38;5;241m.\u001B[39mchunk(\u001B[38;5;241m2\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:317\u001B[0m, in \u001B[0;36mmamba_inner_fn\u001B[0;34m(xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight, out_proj_weight, out_proj_bias, A, B, C, D, delta_bias, B_proj_bias, C_proj_bias, delta_softplus)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmamba_inner_fn\u001B[39m(\n\u001B[1;32m    312\u001B[0m     xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n\u001B[1;32m    313\u001B[0m     out_proj_weight, out_proj_bias,\n\u001B[1;32m    314\u001B[0m     A, B\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, C\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, D\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, delta_bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, B_proj_bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    315\u001B[0m     C_proj_bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, delta_softplus\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    316\u001B[0m ):\n\u001B[0;32m--> 317\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mMambaInnerFn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconv1d_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconv1d_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelta_proj_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mout_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mD\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelta_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mB_proj_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mC_proj_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelta_softplus\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/autograd/function.py:553\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    552\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[1;32m    556\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    557\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    558\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    559\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    560\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    561\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py:115\u001B[0m, in \u001B[0;36mcustom_fwd.<locals>.decorate_fwd\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cast_inputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    114\u001B[0m     args[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_fwd_used_autocast \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mis_autocast_enabled()\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfwd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    117\u001B[0m     autocast_context \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mis_autocast_enabled()\n",
      "File \u001B[0;32m~/Desktop/MasterArbeit/.venv/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:225\u001B[0m, in \u001B[0;36mMambaInnerFn.forward\u001B[0;34m(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight, out_proj_weight, out_proj_bias, A, B, C, D, delta_bias, B_proj_bias, C_proj_bias, delta_softplus, checkpoint_lvl)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m D \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    224\u001B[0m     D \u001B[38;5;241m=\u001B[39m D\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m--> 225\u001B[0m out, scan_intermediates, out_z \u001B[38;5;241m=\u001B[39m \u001B[43mselective_scan_cuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfwd\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconv1d_out\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mD\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelta_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelta_softplus\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    228\u001B[0m ctx\u001B[38;5;241m.\u001B[39mdelta_softplus \u001B[38;5;241m=\u001B[39m delta_softplus\n\u001B[1;32m    229\u001B[0m ctx\u001B[38;5;241m.\u001B[39mout_proj_bias_is_None \u001B[38;5;241m=\u001B[39m out_proj_bias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacity of 9.76 GiB of which 186.50 MiB is free. Process 14850 has 16.10 MiB memory in use. Including non-PyTorch memory, this process has 9.15 GiB memory in use. Of the allocated memory 8.51 GiB is allocated by PyTorch, and 384.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c819458d1592b482"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
